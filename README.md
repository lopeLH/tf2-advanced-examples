# Tensorflow 2 - Advanced examples

This repo contains some notebook-style tensorflow implementations of influential or interesting papers from the field of deep learning. The goal is to highlight the key contributions of these papers, demostrating how they work, but using toy datasets and small models to keep things simple. These implementations are not expected to reproduce the results in the papers or provide a reference implementation, as some details might have been omited to keep focus on the main features.

| Reference | High level description | Notebook
| ------------- |-------------|-------------| 
| Ronneberger, O., Fischer, P., & Brox, T. (2015, October). **U-net: Convolutional networks for biomedical image segmentation.** In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham. [(link)](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28) | **U-net:** Encoder-decoder architecture for semantic segmentation. The encoder uses pooling to reduce spatial dimensions while increasing the number of channels. The decoder uses transposed convolutions to upscale feature maps to the original resolution. Skip-connections from the encoder to the decoder help the decoder by providing local details. | [notebook](https://github.com/lopeLH/tf2-advanced-examples/blob/master/notebooks/Unet.ipynb) | 
| Kingma, D. P., & Welling, M. (2013). **Auto-encoding variational bayes.** arXiv preprint arXiv:1312.6114. [(link)](https://arxiv.org/abs/1312.6114v10) | **Variational Autoencoder:** Autoencoder architecture where the learned encodings are the mean and variance vectors of a multidimensional normal distribution with diagonal covariance. Training is done with a reconstruction loss plus a KL-loss to pull the encodings towards the standard normal. This makes the model learn a continuous embedding space where interpolation between samples is meaningful. Can be used as a generative model.| TBA | 
|  Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. (2017). **Improved Training of Wasserstein GANs**. arXiv preprint arXiv:1704.00028. [(link)](https://arxiv.org/abs/1704.00028) | **WGAN-gp:** Yet another variant of the Generative Adversarial Network framework. The discriminator is replaced by a critic, which tries to pull the fake samples away from the real ones rather than explicitly classifying them. The critic is strongly regularized using a gradient penalty, an trained for several steps before each generator update. This GAN variant provides a better behaved training process than conventional GANs, with a remarkable robustness againts generator collapse mode. | [notebook](https://github.com/lopeLH/tf2-advanced-examples/blob/master/notebooks/WGAN-gp.ipynb) | 

